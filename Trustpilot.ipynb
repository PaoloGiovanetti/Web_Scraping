{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries and modules\n",
    "import pandas as pd\n",
    "from urllib.request import Request\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Trustpilot scraper\n",
    "class trustpilot_product_review_scraper:\n",
    "    \"\"\"Creating a class to scrape all of the Trustpilot reviews\"\"\"\n",
    "    \n",
    "    def __init__(self, trustpilot_site, sleep_time=1):\n",
    "        \"\"\"Initializing url, reviews dictionary, pages to scrape, and starting page\"\"\"\n",
    "        self.url = trustpilot_site\n",
    "        self.reviews_dict = {'names':[],\"time\":[],\"locations\":[],\"titles\":[],\"content\":[],\n",
    "                        \"ratings\":[]}\n",
    "        self.sleep_time = sleep_time\n",
    "        self.pages = [self.url]\n",
    "        self.start_page = 1\n",
    "        \n",
    "        # Calling return_next_page function to collect all of the pages to scrape\n",
    "        print(\"Starting to collect all pages - relax, it will take a moment!\")\n",
    "        self.return_next_page(self.url)\n",
    "        self.end_page = len(self.pages)\n",
    "        \n",
    "    def return_next_page(self,page):\n",
    "        \"\"\"A function that returns to the next page to scrape\"\"\"\n",
    "        req = Request(page, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        html = urlopen(req).read()\n",
    "        bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "        html = urlopen(req).read()\n",
    "        next_pages = (bsObj.find_all(rel=\"next\"))\n",
    "        for page in  next_pages:\n",
    "            if page[\"href\"].startswith(\"ht\"):\n",
    "                next_page = page[\"href\"]\n",
    "                self.pages.append(next_page)\n",
    "                self.sleep_time\n",
    "                self.return_next_page(next_page)    \n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "  \n",
    "    def page_scraper(self, page):\n",
    "        \"\"\"A Function to scrape all of the attributes by one page per time\"\"\"\n",
    "        try:\n",
    "            req = Request(page, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            html = urlopen(req).read()\n",
    "            bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "            html = urlopen(req).read()\n",
    "        \n",
    "            # Creating all lists where to store data scraped\n",
    "            content_list = []\n",
    "            rating_list = []\n",
    "            name_list = []\n",
    "            time_list = []\n",
    "            title_list = []\n",
    "            location_list = []\n",
    "                \n",
    "            # Start scraping one page per time\n",
    "            reviews_ = bsObj.find_all(class_=\"review\")\n",
    "            for review in reviews_:\n",
    "                \n",
    "                # Getting the reviews content\n",
    "                content_ = review.find_all(class_=\"review-content__text\")\n",
    "                if len(content_) == 0:\n",
    "                    content_list.append(\"NaN\")\n",
    "                else:\n",
    "                    content = review.find_all(class_=\"review-content__text\")\n",
    "                    for c in content:\n",
    "                        result = c.get_text(\"\\n\").strip()\n",
    "                        content_list.append(result)\n",
    "            \n",
    "                # Getting reviews titles \n",
    "                titles_ = review.find_all(class_=\"review-content__title\")\n",
    "                if len(titles_) == 0:\n",
    "                    title_list.append(\"NaN\")\n",
    "                else:\n",
    "                    title = review.find_all(class_=\"review-content__title\")\n",
    "                    for t in title:\n",
    "                        result = t.get_text().strip()\n",
    "                        title_list.append(result)\n",
    "                \n",
    "                # Getting reviews time       \n",
    "                times_ = review.find_all('div', {'class' : \"review-content-header\"})\n",
    "                if len(times_) == 0:\n",
    "                    time_list.append(\"NaN\")\n",
    "                else:\n",
    "                    for time in times_:\n",
    "                        result_time = time.find_all(\"script\")[0].string\n",
    "                        result_time = result_time.split('publishedDate\":\"',1)[1]\n",
    "                        result = result_time.split('T',1)[0]\n",
    "                        time_list.append(result)\n",
    "                \n",
    "                # Get reviews ratings\n",
    "                ratings_ = review.find_all(class_=\"star-rating star-rating--medium\")\n",
    "                if len(ratings_) == 0:\n",
    "                    rating_list.append('Nan')\n",
    "                else:\n",
    "                    for rating in ratings_:\n",
    "                        result = rating('img')[0]['alt']\n",
    "                        rating_list.append(result)\n",
    "            \n",
    "                # Getting reviews names\n",
    "                names_ = review.find_all(class_=\"consumer-information__name\")\n",
    "                if len(names_) == 0:\n",
    "                    name_list.append(\"NaN\")\n",
    "                else:\n",
    "                    for name in names_:\n",
    "                        result = name.get_text(\"\\n\").strip()\n",
    "                        name_list.append(result)\n",
    "                      \n",
    "                # Getting reviews locations\n",
    "                locations_ = review.find_all(class_=\"consumer-information__location\")\n",
    "                if len(locations_) == 0:\n",
    "                    location_list.append(\"NaN\")\n",
    "                else:\n",
    "                    for location in locations_:\n",
    "                        result = location(\"span\")[0].string\n",
    "                        location_list.append(result)\n",
    "                \n",
    "            # Adding to the main dictionary\n",
    "            self.reviews_dict['content'].extend(content_list)\n",
    "            self.reviews_dict['ratings'].extend(rating_list)\n",
    "            self.reviews_dict['time'].extend(time_list)\n",
    "            self.reviews_dict['names'].extend(name_list)\n",
    "            self.reviews_dict['locations'].extend(location_list)\n",
    "            self.reviews_dict['titles'].extend(title_list)\n",
    "                \n",
    "        except:\n",
    "            print (\"Not able to scrape page\".format(page), flush=True)\n",
    "                \n",
    "    # Scraping function - main function\n",
    "    def scrape(self):\n",
    "        \"\"\"a Function that returns to a dataframe with all of the reviews scraped\"\"\"\n",
    "        print (\"Total pages to scrape: {}\".format(self.end_page - self.start_page+1), flush=True)\n",
    "        time.sleep(self.sleep_time)\n",
    "        print (\"Start page: {}; End page: {}\".format(self.start_page, self.end_page))\n",
    "        time.sleep(self.sleep_time)\n",
    "        print ()\n",
    "        print (\"Starting to scrape pages!\", flush=True)\n",
    "        time.sleep(self.sleep_time)\n",
    "        \n",
    "        for page in tqdm(self.pages):\n",
    "\n",
    "            self.page_scraper(page)\n",
    "            #\n",
    "            time.sleep(self.sleep_time)\n",
    "\n",
    "        print (\"Completed!\")\n",
    "\n",
    "        # Returning to a Pandas Dataframe\n",
    "        return pd.DataFrame(self.reviews_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to collect all pages - relax, it will take a moment!\n",
      "Total pages to scrape: 23\n",
      "Start page: 1; End page: 23\n",
      "\n",
      "Starting to scrape pages!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab3606cdad7a4588a921c1a902cb85eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=23.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed!\n"
     ]
    }
   ],
   "source": [
    "# Substitute the trustpilot url where you want to start scraping\n",
    "review_scraper = trustpilot_product_review_scraper( # https://de.trustpilot.com/review/ )\n",
    "\n",
    "# Starting to scrape all of the pages and storing them into a dataframe\n",
    "df_reviews = review_scraper.scrape()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the dataframe with all of the reviews scraped\n",
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the DataFrame into Excel\n",
    "df_reviews.to_excel(\"Trustpilot_reviews.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
